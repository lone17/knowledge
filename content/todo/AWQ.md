---
aliases: 
tags: 
created: 2024-02-15 01:43 AM +07:00
modified: 2024-08-26 16:23 PM +07:00
---
#cs/ai/ml/nlp/llm #cs/ai/ml/quantization #todo/read

- paper: [[2306.00978] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)
- repo: [GitHub - mit-han-lab/llm-awq: AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://github.com/mit-han-lab/llm-awq)