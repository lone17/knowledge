---
aliases: 
tags: 
modified: 2024-05-01 03:55 AM +07:00
created: 2024-01-21 18:29 PM +07:00
---
#idea 
from: [Episode 6: Sam Altman - YouTube](https://youtu.be/PkXELH6Y2lM?t=393) - 6:32

# what
- For LLMs, currently we spend the same amount of compute for every token
	- "a dump one or figuring out some complicated math"
	- "*Do the Riemann hypothesis...* is the same as compute as saying *The*"
- Can we implement dynamic computation based on the complexity of the inputs ? #question
	- Can it be generalize for all models not just LLMs ? #question

# related papers
- Mixture of Depth
- RHO-1: Not all tokens are what you need