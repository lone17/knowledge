---
aliases:
  - "@bereska2024"
tags:
  - integration/zotero
modified: 2024-12-04 02:03 AM +07:00
created: 2024-07-23 16:20 PM +07:00
---
> [!Cite]
> Bereska, Leonard, and Efstratios Gavves. _Mechanistic Interpretability for AI Safety -- A Review_. 1, arXiv:2404.14082, arXiv, 22 Apr. 2024. _arXiv.org_, [https://doi.org/10.48550/arXiv.2404.14082](https://doi.org/10.48550/arXiv.2404.14082).

> [!info] Metadata
> **Title**: Mechanistic Interpretability for AI Safety -- A Review
> **Authors**: Leonard Bereska, Efstratios Gavves
> **Cite key**: bereska2024

>[!info] Links
>
> - [Online Link](http://arxiv.org/abs/2404.14082)
> - [Zotero PDF Link](zotero://select/library/items/F3DCV9N9)

> [!info] Abstract
> Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.

# Notes
## From Zotero

## From Obsidian
![[literature notes/papers/Mechanistic Interpretability for AI Safety -- A Review]]
# Annotations
![[Highlight Colour Codings#Highlighting colour codes]]
## From Zotero
**Imported: 2024-12-04**^e0c377

>[!quote] <mark class="hltr-yellow">Note</mark> | [View in local Zotero: page 1](zotero://open-pdf/library/items/MGFTCTVY?page=1&annotation=29LZ2RML) ^29LZ2RML
>“mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding.”


>[!quote] <mark class="hltr-purple">Important</mark> | [View in local Zotero: page 1](zotero://open-pdf/library/items/MGFTCTVY?page=1&annotation=USMX6IZ5) ^USMX6IZ5
>“The interpretability landscape is undergoing a paradigm shift akin to the evolution from behaviorism to cognitive neuroscience in psychology.”


>[!quote] <mark class="hltr-yellow">Note</mark> | [View in local Zotero: page 1](zotero://open-pdf/library/items/MGFTCTVY?page=1&annotation=BF8M7IBI) ^BF8M7IBI
>“Historically, lacking tools for introspection, psychology treated the mind as a black box, focusing solely on observable behaviors. Similarly, interpretability has predominantly relied on black-box techniques, analyzing models based on input-output relationships or using attribution methods that, while probing deeper, still neglect the model’s internal architecture.”


>[!quote] <mark class="hltr-purple">Important</mark> | [View in local Zotero: page 2](zotero://open-pdf/library/items/MGFTCTVY?page=2&annotation=XV6M4CYU) ^XV6M4CYU
>![[assets/Mechanistic Interpretability for AI Safety -- A Review/zotero-image-2-x53-y524.png]]


>[!quote] <mark class="hltr-orange">External Insight</mark> | [View in local Zotero: page 3](zotero://open-pdf/library/items/MGFTCTVY?page=3&annotation=4K3TIV95) ^4K3TIV95
>![[assets/Mechanistic Interpretability for AI Safety -- A Review/zotero-image-3-x67-y588.png]]


>[!quote] <mark class="hltr-orange">External Insight</mark> | [View in local Zotero: page 3](zotero://open-pdf/library/items/MGFTCTVY?page=3&annotation=FP7MX26B) ^FP7MX26B
>“A non-human-centric perspective defines features as independent yet repeatable units that a neural network representation can decompose into (Olah, 2022).”


>[!quote] <mark class="hltr-yellow">Note</mark> | [View in local Zotero: page 4](zotero://open-pdf/library/items/MGFTCTVY?page=4&annotation=U2JZJGQP) ^U2JZJGQP
>“Hypothesis: Superposition  Neural networks represent more features than they have neurons by encoding features in overlapping combinations of neurons.”


>[!quote] <mark class="hltr-purple">Important</mark> | [View in local Zotero: page 4](zotero://open-pdf/library/items/MGFTCTVY?page=4&annotation=I4HVC47B) ^I4HVC47B
>“Non-orthogonality means that features interfere with one another.”


>[!quote] <mark class="hltr-yellow">Note</mark> | [View in local Zotero: page 6](zotero://open-pdf/library/items/MGFTCTVY?page=6&annotation=DDAM3EFV) ^DDAM3EFV
>![[assets/Mechanistic Interpretability for AI Safety -- A Review/zotero-image-6-x67-y429.png]]


>[!quote] <mark class="hltr-yellow">Note</mark> | [View in local Zotero: page 8](zotero://open-pdf/library/items/MGFTCTVY?page=8&annotation=I4IN3NP3) ^I4IN3NP3
>“Definition: Circuit  Circuits are sub-graphs of the network, consisting of features and the weights connecting them.”


