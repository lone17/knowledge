---
aliases:
  - "@cheung2019"
tags:
  - literature-notes/paper
modified: 2024-08-26 16:44 PM +07:00
read: false
read_date: 
created: 2024-07-23 16:20 PM +07:00
---
> [!Cite]
> Cheung, Brian, et al. _Superposition of Many Models into One_. arXiv:1902.05522, arXiv, 17 June 2019. _arXiv.org_, [http://arxiv.org/abs/1902.05522](http://arxiv.org/abs/1902.05522).

> [!info] Metadata
> **Title**: Superposition of many models into one
> **Authors**: Brian Cheung, Alex Terekhov, Yubei Chen, Pulkit Agrawal, Bruno Olshausen
> **Cite key**: cheung2019

>[!info] Links
> - [Online Link](http://arxiv.org/abs/1902.05522)
> - [Zotero PDF Link](zotero://select/library/items/UEG6TRG3)

> [!info] Abstract
> We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without signiÔ¨Åcantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.
# Annotations
## Zotero

## PDF++