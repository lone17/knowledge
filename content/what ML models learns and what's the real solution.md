---
aliases: 
tags: 
modified: 2024-10-13 17:26 PM +07:00
created: 2024-09-29 15:24 PM +07:00
---
#thought

_Thought while listening to [[Open Problems in the Theory of Deep Learning - MITCBMM]]_
- timestamp: [Panel Discussion: Open Problems in the Theory of Deep Learning - YouTube](https://youtu.be/bpZuTCj6Jtk?t=2498)

In [[The Bitter Lesson]] where it says ![[The Bitter Lesson#^d5f858]]
So if using our inductive biases (the researcher tries to incorporate their knowledge into the agent) gives worse results than statistically model a large amount of data, does it mean that our inductive biases are wrong ?
For example, we came up with theory of language and from that derive syntax trees to analyze text, but language modelling using statistical methods outperform that. Does that mean our theory of language is wrong ? Can we make such deduction ? #question

Consequently, does this mean that the solution produce by the statistical model is the true solution ? Or is it just an approximation ? What define a real solution and does one exist ? Probably not. #question

Does a good approximation lie in a path toward the true solution ? In other words, if an approximation is good (i.e. produce good empirical results), does it similar to the real solution ? Could it be that those approximations exists in a whole different space than the real solution, it just happens to give good results but will never reach the best possible results. #question